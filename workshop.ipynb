{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Aritificial Intelligence Workshop\n",
    "#### Women in Tech Summit 8th February 2020\n",
    "####  by Alise Midtfjord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainable Artificial Intelligence (XAI) refers to methods that aim at making Artificial Intelligence systems (AI) more transparent, such that their decisions can be understood by humans. It it the opposite of the concept of \"black box\", where even the AI developers can't explain why the AI system arrived at a specific decision. We can think if XAI as in implementation of the social right to explanation. \n",
    "\n",
    "In this workshop/notebook, we are going to use deep learning to classify a chosen text as positive, negative or neutral, on the basis of it's content. After the classification, we are going to use an method called LRP (Layer-wise Relevance Propagation) to explain why the deep learning system came to it's decision, making it an Explainable Ariticial Intelligence system (XAI) instead of a black-box system.\n",
    "\n",
    "The LRP implementation is based on the following papers:\n",
    "- [https://doi.org/10.1371/journal.pone.0130140](https://doi.org/10.1371/journal.pone.0130140)\n",
    "- [https://doi.org/10.18653/v1/W17-5221](https://doi.org/10.18653/v1/W17-5221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the text you want to classify\n",
    "First, we are going to enter the text we want to classify, and make it compatible with the Neural Network. We must do a few imports. Make sure the LSTM_bibi.py and heatmap.py files are in the same folder as this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_bidi import *                      # the LSTM_bidi file containts functions for processing a text, performing \n",
    "                                             # sentiment analysis with deep learning and explaining the result with LRP\n",
    "    \n",
    "from heatmap import html_heatmap             # the heatmap file containt the functions for converting the relevances\n",
    "                                             # obtained by a LRP to readable heatmaps\n",
    "\n",
    "import codecs                                # codecs is a package used to code or decodes text to bytes\n",
    "import numpy as np                           # NumPy is the fundamental package for array computing with Python\n",
    "from IPython.display import display, HTML    # IPython.display is a public API for display tools in IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: Write the text you want to classify between the apostrophes. It can be as long as you want, but it must be in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Write/paste your text here  \"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: Now we must make the text compatible with the Neural Network. Use the python commands lower(), replace() and split() to do the following:\n",
    "\n",
    "- Make all characers into lowercase characters\n",
    "- Replace all symbols with space (e.g. comma, exclamation mark, question mark etc.)\n",
    "- Split the text into a list using whitespace as separator. \n",
    "\n",
    "Here are the commands:\n",
    "\n",
    "**text.lower()** \n",
    "The string lower() method converts all uppercase characters in a string into lowercase characters and returns it. Link: https://www.programiz.com/python-programming/methods/string/lower\n",
    "\n",
    "**text.replace(‘old’,’new ’)**\n",
    "Python string method replace() returns a copy of the string in which the occurrences of old have been replaced with new. Link: https://www.tutorialspoint.com/python/string_replace.htm\n",
    "\n",
    "**text.split()** \n",
    "The split() method splits a string into a list. You can specify the separator; default separator is any whitespace. Link: https://www.w3schools.com/python/ref_string_split.asp\n",
    "\n",
    "Example: \n",
    "text = text.replace('!', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = # Write your code here\n",
    "text = # Write your code here\n",
    "text = # Write your code here\n",
    "text = # Write your code here\n",
    "text = # Write your code here\n",
    "text = # Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: Print out the list containing your text. You can read how to print here: https://www.w3schools.com/python/ref_func_print.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Neural Network is trained on the dataset Stanford Sentiment Treebank (SST), it only recognizes words that are in this list. Therefore, our text can not have words that are not in this dataset. We must define a function remove_invalid_words to remove all words in our text that are not found in the Stanford Sentiment Treebank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_words(text):\n",
    "    \"\"\"Removes all words from text that are not in the Stanford Sentiment Treebank dataset\"\"\"\n",
    "    net  = LSTM_bidi()         # load in the trained neural network\n",
    "    words = text.copy()        # create a copy of the text\n",
    "    for w in text:             # remove all words that are not in the Standord Sentiment Treebank\n",
    "        if w not in net.voc:\n",
    "            words.remove(w)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: Call the function remove_invalid_words and name the new list *words*. Here you can read how to call a function: https://www.w3schools.com/python/python_functions.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = # Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: Print out the list containing your text again (which has now changed name to *words*), and you can see which words got removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the text classification with a Neural Network\n",
    "Now we are going to classify our text with a Neural Network, more specificly a bidirectional Long short-term memory (LSTM). This is an artificial recurrent neural network (RNN) architecure, which are good at processing not only single data points but also entire sequences of data (such as text or video). It is therefore often used for Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to create the classes in which the text can be classied as. The sentiment classes are encoded the following way:  \n",
    "**0 = Very negative, 1 = Negative, 2 = Neutral, 3 = Positive, 4 = Very positive**\n",
    "\n",
    "**Edit**:  Create a list calles sentiment_coding that consists of the 5 elements with the text \"Very negative\", \"Negative\" etc. in the right order from 0-4. You can read how to create a list consisting of text-elements (called strings) here: https://www.w3schools.com/python/python_lists.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_coding = # Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial intelligence we are going to use to classify our text is a Neural Network, more specificly a bidirectional Long short-term memory (LSTM). This is an artificial recurrent neural network (RNN) architecure, which are good at processing not only single data points but also entire sequences of data (such as text or video). It is therefore often used for Natural Language Processing (NLP). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function *predict* which uses the Neural Network LSTM by calling the function LSTM_bidi(), which we imported in the beginning. We call the LSTM Neural Network *net*, and the network is already trained on the Stanford Sentiment Treebank (SST) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(words):\n",
    "    \"\"\"Returns the classifier's predicted class\"\"\"\n",
    "    net                 = LSTM_bidi()                                   # load trained LSTM model\n",
    "    w_indices           = [net.voc.index(w) for w in words]             # convert input sentence to word IDs\n",
    "    net.set_input(w_indices)                                            # set LSTM input sequence\n",
    "    scores              = net.forward()                                 # classification prediction scores\n",
    "    return np.argmax(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: Predict the sentiment of your text (now called *words*) by calling the function \"predict\" defined in the previous box, and name the prediction *predicted_class*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class =  # Write your code here                                                   # get predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit:** Print out the predicted class of your text. If you find it possible, try also to print out the predicted class by text (e.g. *Very negative*), using the list sentiment_coding which you created earlier. You can see how to access items from lists here, under *Access items*: https://www.w3schools.com/python/python_lists.asp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the text classification by computing LRP relevances\n",
    "Now we have used a neural network to get the classification/sentiment of our text. Then, we need to find out why the nerural network came to this decision, and here we will use a method called Layer-wise Relevance Propagation. We will start by setting it's hyperparamteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible edit**: Here you can change your hyperparamteres *eps* or *bias_factor* if you want to. They are hyperparameters used in the LRP relevance calculations. In the beginning, it might be smart just to leave it as it is. *eps* is a threshold value for how large the relevance for the words needs to be in order to be shown. If their relevance is below eps, they are set to zero. *bias_factor* is the size of the bias included in the caculation of the relevance. It is recommended to set this as 0, as this leads to that the total amount of relevance is conserved for each layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRP hyperparameters:\n",
    "eps                 = 0.001                                                  # small positive number\n",
    "bias_factor         = 0.0                                                    # recommended value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: Load in the trained Neural Network from the import LSTM_bidi() and call it *net*. If you need a hint, look at the first line if code in the functions *remove_invalid_words* and *predict* which we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs the LRP on the classification done by the neural network on you text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_indices           = [net.voc.index(w) for w in words]                      # convert input sentence to word IDs\n",
    "Rx, Rx_rev, R_rest  = net.lrp(w_indices, predicted_class, eps, bias_factor)  # perform LRP\n",
    "R_words             = np.sum(Rx + Rx_rev, axis=1)                            # compute word-level LRP relevances\n",
    "\n",
    "scores              = net.s.copy()                                           # classification prediction scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit** Print out the predicted class of your text (0-4). If you find it possible, try also to print out the predicted class by text, using the list sentiment_coding which you created earlier. Print also out the scores for you classification, meaning the possibility of each class. The variable containing the scores is called *scores*, and is defined in the previous box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints out the relevance of each word in you text has in the classification. Words marked in red are words that contributed to the predicted class, while words marked in blue are words that had a negative contribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words)))                                    # display the heat map of relevances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit**: It is also possible to take a look at the other classes, to understand why these class was not chosen. Change the value of the *target_class* to the class you are curious about, and you will get a similar heat map for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 1\n",
    "\n",
    "w_indices           = [net.voc.index(w) for w in words]                      # convert input sentence to word IDs\n",
    "Rx, Rx_rev, R_rest  = net.lrp(w_indices, target_class, eps, bias_factor)     # perform LRP\n",
    "R_words             = np.sum(Rx + Rx_rev, axis=1)                            # compute word-level LRP relevances\n",
    "\n",
    "scores              = net.s.copy()                                           # classification prediction scores\n",
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words)))                                  # display the heat map of relevances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! Not only have you classified a text by it's content using deep learning, you have also managed to explain how the AI system came to it's decision. You have made a black-box algorithms explainable, and contributed to make the future AI-powered World more ethical and just!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra task 1:** LRP can be used on more AI systems than sentiment analysis. Check out this demo, where you can use the same implementation of LRP on Handwriting Classification, Image Classification and Text Classification: https://lrpserver.hhi.fraunhofer.de/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra task 2:** Use the LRP to identify important words, and then remove them to see how the prediction changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra task 3:** Try other texts with different sentiment and contents. Try to fool the Neural Network with difficult texts, and then use the LRP to see what made it make a wrong prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
